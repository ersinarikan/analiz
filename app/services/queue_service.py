import threading
import queue
import logging
import time
import subprocess
import sys
import os
import errno
import fcntl
from flask import current_app
import traceback
from contextlib import contextmanager
from typing import Tuple

logger = logging.getLogger(__name__)

# Queue backend
# - "memory": mevcut in-process queue (tek proses iÃ§in uygun)
# - "redis": web ve worker proseslerini ayÄ±rmak iÃ§in (Ã¶nerilen prod)
QUEUE_BACKEND = os.environ.get("WSANALIZ_QUEUE_BACKEND", "redis").strip().lower()
REDIS_URL = os.environ.get("WSANALIZ_REDIS_URL", "redis://localhost:6379/0").strip()
REDIS_QUEUE_KEY = os.environ.get("WSANALIZ_QUEUE_KEY", "wsanaliz:analysis_queue").strip()
REDIS_WORKER_ACTIVE_KEY = os.environ.get("WSANALIZ_WORKER_ACTIVE_KEY", "wsanaliz:worker:active_analyses").strip()
REDIS_WORKER_PROCESSING_KEY = os.environ.get("WSANALIZ_WORKER_PROCESSING_KEY", "wsanaliz:worker:is_processing").strip()
REDIS_WORKER_HEARTBEAT_KEY = os.environ.get("WSANALIZ_WORKER_HEARTBEAT_KEY", "wsanaliz:worker:last_heartbeat").strip()

_redis_client = None


def is_redis_backend() -> bool:
    return QUEUE_BACKEND == "redis"


def _get_redis():
    global _redis_client
    if _redis_client is not None:
        return _redis_client
    try:
        import redis  # type: ignore

        _redis_client = redis.Redis.from_url(REDIS_URL, decode_responses=True)
        return _redis_client
    except Exception as e:
        raise RuntimeError(f"Redis queue backend seÃ§ildi ama redis client init edilemedi: {e}")


def _set_worker_state(is_processing_value: bool, active_analyses: int):
    """Worker state'i Redis'e yazar (queue stats endpoint iÃ§in)."""
    if not is_redis_backend():
        return
    try:
        r = _get_redis()
        pipe = r.pipeline()
        pipe.set(REDIS_WORKER_PROCESSING_KEY, "1" if is_processing_value else "0", ex=60)
        pipe.set(REDIS_WORKER_ACTIVE_KEY, str(active_analyses), ex=60)
        pipe.set(REDIS_WORKER_HEARTBEAT_KEY, str(time.time()), ex=60)
        pipe.execute()
    except Exception as e:
        logger.warning(f"Worker state Redis'e yazÄ±lamadÄ±: {e}")


# Global analiz kuyruÄŸu (memory backend)
analysis_queue = queue.Queue()
# Ä°ÅŸleme kilidi (memory backend)
processing_lock = threading.Lock()
is_processing = False

_GPU_LOCK_PATH = os.environ.get("WSANALIZ_GPU_LOCK_PATH", "/tmp/wsanaliz_gpu_analysis.lock")


def _acquire_gpu_lock():
    """
    Cross-process GPU lock.

    Toplu analizde birden fazla Gunicorn worker aynÄ± anda subprocess baÅŸlatÄ±p
    CUDA OOM'a neden olabiliyordu. Bu lock, tÃ¼m prosesler arasÄ±nda aynÄ± anda
    sadece 1 analiz subprocess'inin GPU Ã¼zerinde Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.

    Returns:
        file descriptor (must be kept open to hold the lock)
    """
    fd = os.open(_GPU_LOCK_PATH, os.O_CREAT | os.O_RDWR, 0o644)

    # Non-blocking acquire loop (eventlet uyumlu)
    while True:
        try:
            fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            return fd
        except OSError as e:
            if e.errno not in (errno.EAGAIN, errno.EACCES):
                os.close(fd)
                raise

            # Lock busy -> yield/sleep
            try:
                import eventlet  # type: ignore

                eventlet.sleep(0.5)
            except Exception:
                time.sleep(0.5)


def _release_gpu_lock(fd: int):
    try:
        fcntl.flock(fd, fcntl.LOCK_UN)
    finally:
        try:
            os.close(fd)
        except Exception:
            pass

@contextmanager
def database_session(app):
    """
    Thread-safe database session context manager
    Her thread iÃ§in ayrÄ± session yÃ¶netimi saÄŸlar
    """
    try:
        with app.app_context():
            from app import db
            
            # Yeni session baÅŸlat
            session = db.session
            
            # Ä°ÅŸlem baÅŸlangÄ±cÄ±nda session'Ä± temizle
            session.rollback()  # Ã–nceki iÅŸlemlerden kalan uncommitted changes'i temizle
            session.close()     # Connection pool'a geri dÃ¶ndÃ¼r
            
            # Fresh session baÅŸlat
            yield session
            
            # BaÅŸarÄ±lÄ± iÅŸlem sonrasÄ± commit
            session.commit()
            
    except Exception as e:
        # Hata durumunda rollback
        try:
            session.rollback()
            logger.error(f"Database session rollback yapÄ±ldÄ±: {str(e)}")
        except:
            pass
        raise
        
    finally:
        # Her durumda session'Ä± temizle
        try:
            session.close()
        except:
            pass

def add_to_queue(analysis_id):
    """
    Analiz iÅŸini kuyruÄŸa ekler ve iÅŸleyiciyi baÅŸlatÄ±r
    
    Args:
        analysis_id: Eklenecek analiz ID'si
    """
    logger.info(f"Analiz kuyruÄŸa ekleniyor: {analysis_id}")
    if is_redis_backend():
        # Cross-process queue: web sadece enqueue eder; worker BLPOP ile tÃ¼ketir.
        r = _get_redis()
        r.rpush(REDIS_QUEUE_KEY, str(analysis_id))
        emit_queue_status()
        return

    # Fallback: in-process queue (dev)
    analysis_queue.put(analysis_id)
    start_processor()
    emit_queue_status()

def emit_queue_status():
    """
    WebSocket ile kuyruk durum bilgilerini istemcilere gÃ¶nderir
    """
    try:
        status = get_queue_status()
        
        # WebSocket ile kuyruk durumu bildirimi gÃ¶nder
        try:
            from app.socketio_instance import get_socketio
            running_socketio = get_socketio()
            if running_socketio:
                running_socketio.emit('queue_status', status)
                logger.debug(f"Kuyruk durumu WebSocket ile gÃ¶nderildi: {status}")
        except Exception as ws_err:
            logger.warning(f"WebSocket kuyruk durumu bildirimi hatasÄ±: {str(ws_err)}")
            
        logger.debug(f"Kuyruk durumu mevcut: {status}")
        # HTTP endpoint /api/queue/status hala mevcut
    except Exception as e:
        logger.warning(f"Kuyruk durumu gÃ¼ncellemesi hatasÄ±: {str(e)}")

def start_processor():
    """
    Kuyruk iÅŸleyici thread'i baÅŸlatÄ±r (henÃ¼z Ã§alÄ±ÅŸmÄ±yorsa)
    """
    global is_processing
    if is_redis_backend():
        # Redis backend'te queue processing ayrÄ± worker process'te yapÄ±lÄ±r.
        logger.info("Redis queue backend aktif: in-process queue processor baÅŸlatÄ±lmÄ±yor.")
        return
    with processing_lock:
        if not is_processing:
            is_processing = True
            # Gunicorn eventlet worker altÄ±nda OS thread yerine eventlet greenlet kullanmak daha gÃ¼venli.
            # Ã–zellikle SocketIO emit'leri background task iÃ§inde Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in, eventlet hub ile uyumlu olmalÄ±.
            try:
                import eventlet  # type: ignore
                eventlet.spawn_n(process_queue)
                logger.info("Kuyruk iÅŸleyici baÅŸlatÄ±ldÄ± (eventlet greenlet)")
            except Exception:
                thread = threading.Thread(target=process_queue)
                thread.daemon = True
                thread.start()
                logger.info("Kuyruk iÅŸleyici baÅŸlatÄ±ldÄ± (thread)")

def process_queue():
    """
    Kuyruktan sÄ±rayla analiz iÅŸlerini iÅŸler - Thread-safe database management ile
    """
    global is_processing
    
    try:
        # Ana Flask app'i globalden al ve context aÃ§
        from app import global_flask_app, db
        from app.socketio_instance import get_socketio
        logger.info("Kuyruk iÅŸleyici Ã§alÄ±ÅŸÄ±yor. Global Flask app context aÃ§Ä±lÄ±yor.")
        with global_flask_app.app_context():
            while not analysis_queue.empty():
                # Kuyruk durumu bildirimi gÃ¶nder
                emit_queue_status()
                analysis_id = analysis_queue.get()
                logger.info(f"Analiz iÅŸleme baÅŸlÄ±yor: #{analysis_id}, Kalan iÅŸler: {analysis_queue.qsize()}")
                try:
                    process_one_analysis(str(analysis_id), global_flask_app)
                finally:
                    analysis_queue.task_done()
                    emit_queue_status()
                    try:
                        import eventlet  # type: ignore
                        eventlet.sleep(1)
                    except Exception:
                        time.sleep(1)
            
            logger.info("TÃ¼m analizler tamamlandÄ±, kuyruk boÅŸ.")
            
            # Son kuyruk durumu bildirimi
            emit_queue_status()
            
    except Exception as e:
        logger.error(f"Kuyruk iÅŸleyici genel hatasÄ±: {str(e)}", exc_info=True)
        
    finally:
        # Ä°ÅŸleme durumunu sÄ±fÄ±rla
        with processing_lock:
            is_processing = False
            logger.info("Kuyruk iÅŸleyici durduruldu.")
            
            # EÄŸer kuyrukta hala eleman varsa, yeni bir iÅŸleyici baÅŸlat
            if not analysis_queue.empty():
                start_processor()


def process_one_analysis(analysis_id: str, app=None) -> Tuple[bool, str]:
    """
    Tek bir analysis_id iÃ§in analizi Ã§alÄ±ÅŸtÄ±rÄ±r (subprocess izolasyonu + GPU lock).
    Hem in-process queue (dev) hem de ayrÄ± worker proses (prod) tarafÄ±ndan kullanÄ±lÄ±r.
    """
    # Worker state (redis) - best effort
    _set_worker_state(True, 1)

    from app import global_flask_app
    target_app = app or global_flask_app
    if target_app is None:
        raise RuntimeError("Flask app bulunamadÄ± (global_flask_app None). create_app() Ã§aÄŸrÄ±lmÄ±ÅŸ olmalÄ±.")

    try:
        with target_app.app_context():
            # Analizin varlÄ±ÄŸÄ±nÄ±/iptalini kontrol et
            from app.models.analysis import Analysis
            analysis_file_id = None

            with database_session(target_app) as session:
                analysis = Analysis.query.get(analysis_id)
                if not analysis:
                    logger.error(f"Analiz bulunamadÄ±: {analysis_id}")
                    return False, "Analiz bulunamadÄ±"
                analysis_file_id = analysis.file_id
                if getattr(analysis, "is_cancelled", False):
                    logger.info(f"ðŸš« Analiz #{analysis_id} iptal edilmiÅŸ, atlanÄ±yor")
                    return False, "Analiz iptal edildi"

                # Mark as processing + set start_time for observability.
                try:
                    if getattr(analysis, "status", None) != "processing":
                        analysis.status = "processing"
                    if not getattr(analysis, "start_time", None):
                        from datetime import datetime
                        analysis.start_time = datetime.utcnow()
                except Exception:
                    pass

            start_time = time.time()
            gpu_lock_fd = None
            try:
                logger.info(f"ðŸ”’ GPU lock bekleniyor (analysis_id={analysis_id})")
                gpu_lock_fd = _acquire_gpu_lock()
                logger.info(f"ðŸ”“ GPU lock alÄ±ndÄ± (analysis_id={analysis_id})")

                # NOTE:
                # subprocess.run(...) blocks for the whole duration of the analysis.
                # Our worker heartbeat keys in Redis have TTL (ex=60). For longer analyses (videos),
                # the keys expire and /api/queue/stats shows worker_last_heartbeat=null although
                # the worker is alive and processing. Use Popen + poll loop to refresh heartbeat.
                logs_dir = os.environ.get("WSANALIZ_SUBPROCESS_LOG_DIR", "/opt/wsanaliz/logs")
                os.makedirs(logs_dir, exist_ok=True)
                stdout_path = os.path.join(logs_dir, f"analysis_subprocess_{analysis_id}.stdout.log")
                stderr_path = os.path.join(logs_dir, f"analysis_subprocess_{analysis_id}.stderr.log")

                proc = subprocess.Popen(
                    [sys.executable, "-m", "app.services.analysis_subprocess_runner", str(analysis_id)],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                )

                stdout_lines: list[str] = []
                stderr_lines: list[str] = []
                stdout_file = None
                stderr_file = None

                try:
                    stdout_file = open(stdout_path, "w", encoding="utf-8")
                    stderr_file = open(stderr_path, "w", encoding="utf-8")
                except Exception:
                    stdout_file = None
                    stderr_file = None

                def _stream_reader(stream, lines, file_obj):
                    try:
                        for line in iter(stream.readline, ""):
                            lines.append(line.rstrip("\n"))
                            if file_obj:
                                file_obj.write(line)
                                file_obj.flush()
                    except Exception:
                        pass
                    finally:
                        try:
                            stream.close()
                        except Exception:
                            pass

                if proc.stdout is not None:
                    threading.Thread(
                        target=_stream_reader,
                        args=(proc.stdout, stdout_lines, stdout_file),
                        daemon=True,
                    ).start()
                if proc.stderr is not None:
                    threading.Thread(
                        target=_stream_reader,
                        args=(proc.stderr, stderr_lines, stderr_file),
                        daemon=True,
                    ).start()

                start_wait = time.time()
                while True:
                    rc = proc.poll()
                    if rc is not None:
                        break

                    # Refresh heartbeat while processing (best effort)
                    _set_worker_state(True, 1)

                    # Hard timeout (1 hour)
                    if (time.time() - start_wait) > (60 * 60):
                        try:
                            proc.kill()
                        except Exception:
                            pass
                        raise subprocess.TimeoutExpired(cmd=proc.args, timeout=60 * 60)

                    time.sleep(5)

                # Ensure process is fully terminated and streams drained
                try:
                    proc.wait(timeout=5)
                except Exception:
                    pass

                # NOTE: Some native deps (e.g. insightface/onnxruntime) can print to stdout,
                # which can break the "last-line-is-JSON" assumption. Parse the LAST valid JSON
                # object we can find in stdout, and fall back to stderr if needed.
                # We already streamed outputs into stdout_lines/stderr_lines.

                try:
                    if stdout_file:
                        stdout_file.flush()
                        stdout_file.close()
                    if stderr_file:
                        stderr_file.flush()
                        stderr_file.close()
                except Exception:
                    pass

                def _extract_last_json(lines: list[str]):
                    import json as _json

                    last_obj = None
                    for line in lines:
                        line_s = (line or "").strip()
                        if not line_s:
                            continue
                        if not (line_s.startswith("{") and line_s.endswith("}")):
                            continue
                        try:
                            obj = _json.loads(line_s)
                            if isinstance(obj, dict) and ("success" in obj or "message" in obj):
                                last_obj = obj
                        except Exception:
                            continue
                    return last_obj

                out = _extract_last_json(stdout_lines) or _extract_last_json(stderr_lines)
                if out is not None:
                    success = bool(out.get("success", False))
                    message = str(out.get("message", ""))
                else:
                    # Give a helpful debug snippet (last non-empty line).
                    last_line = ""
                    for line in reversed(stdout_lines):
                        if (line or "").strip():
                            last_line = (line or "").strip()
                            break
                    if not last_line:
                        for line in reversed(stderr_lines):
                            if (line or "").strip():
                                last_line = (line or "").strip()
                                break
                    success = False
                    message = f"Subprocess Ã§Ä±ktÄ± parse edilemedi (rc={proc.returncode}). Son satÄ±r: {last_line[:400]}"
            except subprocess.TimeoutExpired:
                success = False
                message = "Analiz subprocess timeout (1 saat)"
            except Exception as sub_err:
                success = False
                message = f"Analiz subprocess baÅŸlatÄ±lamadÄ±: {sub_err}"
            finally:
                if gpu_lock_fd is not None:
                    _release_gpu_lock(gpu_lock_fd)

            elapsed_time = time.time() - start_time
            logger.info(
                f"Analiz #{analysis_id} tamamlandÄ±: {'BaÅŸarÄ±lÄ±' if success else 'BaÅŸarÄ±sÄ±z'}, "
                f"SÃ¼re: {elapsed_time:.2f}s, Mesaj: {message}"
            )

            # Final durumu gÃ¼ncelle
            with database_session(target_app) as session:
                analysis = Analysis.query.get(analysis_id)
                if analysis:
                    from datetime import datetime

                    if success:
                        analysis.status = "completed"
                    else:
                        analysis.status = "failed"
                        # keep message for UI/debugging
                        try:
                            analysis.error_message = message
                        except Exception:
                            pass

                    if not getattr(analysis, "end_time", None):
                        analysis.end_time = datetime.utcnow()

            _emit_analysis_completion(analysis_id, analysis_file_id, success, elapsed_time, message)
            return success, message

    except Exception as e:
        logger.error(f"Analiz iÅŸleme hatasÄ±: #{analysis_id}, {str(e)}")
        logger.error(traceback.format_exc())
        try:
            from app.models.analysis import Analysis
            with database_session(target_app) as session:
                analysis = Analysis.query.get(analysis_id)
                error_analysis_file_id = analysis.file_id if analysis else None
                if analysis:
                    from datetime import datetime
                    analysis.status = 'failed'
                    try:
                        analysis.error_message = f"Ä°ÅŸlem hatasÄ±: {str(e)}"
                    except Exception:
                        pass
                    if not getattr(analysis, "end_time", None):
                        analysis.end_time = datetime.utcnow()
            _emit_analysis_completion(analysis_id, error_analysis_file_id, False, 0, f"Ä°ÅŸlem hatasÄ±: {str(e)}")
        except Exception as db_err:
            logger.error(f"Hata durumunda DB gÃ¼ncelleme hatasÄ±: {str(db_err)}")
        return False, str(e)
    finally:
        _set_worker_state(False, 0)

def _emit_analysis_status(analysis_id, file_id, status, progress, message):
    """Analiz durumu WebSocket bildirimi (eski fonksiyon - artÄ±k kullanÄ±lmÄ±yor)"""
    try:
        # Bu fonksiyon artÄ±k kullanÄ±lmÄ±yor - yeni WebSocket sistem aktif
        logger.info(f"Analiz durumu gÃ¼ncellendi: {analysis_id} - {status} ({progress}%)")
        
    except Exception as e:
        logger.warning(f"Analiz durumu gÃ¼ncelleme hatasÄ±: {str(e)}")

def _emit_analysis_completion(analysis_id, file_id, success, elapsed_time, message):
    """Analiz tamamlanma WebSocket bildirimi"""
    try:
        from app.routes.websocket_routes import emit_analysis_completed
        status_text = "completed" if success else "failed"
        final_message = f"Analiz {status_text} ({elapsed_time:.2f}s): {message}"
        
        emit_analysis_completed(analysis_id, final_message, file_id)
        logger.info(f"Analiz tamamlandÄ± - WebSocket bildirimi: {analysis_id} - {status_text} ({elapsed_time:.2f}s)")
        
    except Exception as e:
        logger.warning(f"Analiz tamamlanma WebSocket bildirimi hatasÄ±: {str(e)}")

def remove_cancelled_from_queue(app=None):
    """
    Kuyruktaki iptal edilmiÅŸ analizleri temizler
    
    Returns:
        int: Temizlenen analiz sayÄ±sÄ±
    """
    try:
        # Prefer existing Flask app context if available; otherwise use provided app or global fallback.
        from flask import current_app as _current_app, has_app_context
        target_app = None
        if has_app_context():
            target_app = _current_app
        else:
            try:
                from app import global_flask_app as _global_flask_app
            except Exception:
                _global_flask_app = None
            target_app = app or _global_flask_app

        if target_app is None:
            logger.warning("remove_cancelled_from_queue: Flask app bulunamadÄ± (no app_context, app param None, global_flask_app None)")
            return 0

        if is_redis_backend():
            # Redis list Ã¼zerinde basit bir filtreleme (kÃ¼Ã§Ã¼k kuyruklarda yeterli)
            from app.models.analysis import Analysis
            r = _get_redis()
            removed_count = 0

            if has_app_context():
                items = r.lrange(REDIS_QUEUE_KEY, 0, -1) or []
                kept = []
                for analysis_id in items:
                    analysis = Analysis.query.get(analysis_id)
                    if analysis and analysis.is_cancelled:
                        removed_count += 1
                    else:
                        kept.append(analysis_id)

                if removed_count:
                    pipe = r.pipeline()
                    pipe.delete(REDIS_QUEUE_KEY)
                    if kept:
                        pipe.rpush(REDIS_QUEUE_KEY, *kept)
                    pipe.execute()
            else:
                with target_app.app_context():
                    items = r.lrange(REDIS_QUEUE_KEY, 0, -1) or []
                    kept = []
                    for analysis_id in items:
                        analysis = Analysis.query.get(analysis_id)
                        if analysis and analysis.is_cancelled:
                            removed_count += 1
                        else:
                            kept.append(analysis_id)

                    if removed_count:
                        pipe = r.pipeline()
                        pipe.delete(REDIS_QUEUE_KEY)
                        if kept:
                            pipe.rpush(REDIS_QUEUE_KEY, *kept)
                        pipe.execute()

            if removed_count:
                logger.info(f"âœ… Redis kuyruÄŸundan {removed_count} iptal edilmiÅŸ analiz temizlendi")
            return removed_count

        from app.models.analysis import Analysis
        
        removed_count = 0
        temp_queue = queue.Queue()
        
        # Kuyruktaki tÃ¼m analizleri kontrol et
        if has_app_context():
            while not analysis_queue.empty():
                try:
                    analysis_id = analysis_queue.get_nowait()
                    
                    # Analizin iptal edilip edilmediÄŸini kontrol et
                    analysis = Analysis.query.get(analysis_id)
                    if analysis and analysis.is_cancelled:
                        logger.info(f"ðŸ—‘ï¸ Kuyruktan iptal edilmiÅŸ analiz temizlendi: #{analysis_id}")
                        removed_count += 1
                    else:
                        # Ä°ptal edilmemiÅŸse geri kuyruÄŸa koy
                        temp_queue.put(analysis_id)
                        
                except queue.Empty:
                    break
                except Exception as e:
                    logger.error(f"Kuyruk temizleme hatasÄ±: {str(e)}")
                    break
            
            # TemizlenmiÅŸ kuyruÄŸu geri yÃ¼kle
            while not temp_queue.empty():
                analysis_queue.put(temp_queue.get())
        else:
            with target_app.app_context():
                while not analysis_queue.empty():
                    try:
                        analysis_id = analysis_queue.get_nowait()

                        # Analizin iptal edilip edilmediÄŸini kontrol et
                        analysis = Analysis.query.get(analysis_id)
                        if analysis and analysis.is_cancelled:
                            logger.info(f"ðŸ—‘ï¸ Kuyruktan iptal edilmiÅŸ analiz temizlendi: #{analysis_id}")
                            removed_count += 1
                        else:
                            # Ä°ptal edilmemiÅŸse geri kuyruÄŸa koy
                            temp_queue.put(analysis_id)

                    except queue.Empty:
                        break
                    except Exception as e:
                        logger.error(f"Kuyruk temizleme hatasÄ±: {str(e)}")
                        break

                # TemizlenmiÅŸ kuyruÄŸu geri yÃ¼kle
                while not temp_queue.empty():
                    analysis_queue.put(temp_queue.get())
        
        if removed_count > 0:
            logger.info(f"âœ… Kuyruktan {removed_count} iptal edilmiÅŸ analiz temizlendi")
            
        return removed_count
        
    except Exception as e:
        logger.error(f"âŒ Kuyruk temizleme hatasÄ±: {str(e)}")
        return 0

def get_queue_status():
    """
    Kuyruk durumu bilgilerini dÃ¶ndÃ¼rÃ¼r
    
    Returns:
        dict: Kuyruk durum bilgileri
    """
    if is_redis_backend():
        try:
            r = _get_redis()
            qsize = int(r.llen(REDIS_QUEUE_KEY) or 0)
            is_proc = (r.get(REDIS_WORKER_PROCESSING_KEY) or "0") == "1"
            return {
                'queue_size': qsize,
                'is_processing': is_proc,
                'timestamp': time.time()
            }
        except Exception as e:
            logger.warning(f"Redis queue status okunamadÄ±: {e}")
            return {
                'queue_size': 0,
                'is_processing': False,
                'timestamp': time.time(),
                'error': str(e)
            }
    return {
        'queue_size': analysis_queue.qsize(),
        'is_processing': is_processing,
        'timestamp': time.time()
    }

def get_queue_stats():
    """
    Kuyruk istatistiklerini dÃ¶ndÃ¼rÃ¼r
    
    Returns:
        dict: Kuyruk istatistikleri
    """
    if is_redis_backend():
        try:
            r = _get_redis()
            qsize = int(r.llen(REDIS_QUEUE_KEY) or 0)
            is_proc = (r.get(REDIS_WORKER_PROCESSING_KEY) or "0") == "1"
            active = int(r.get(REDIS_WORKER_ACTIVE_KEY) or "0")
            heartbeat = r.get(REDIS_WORKER_HEARTBEAT_KEY)
            return {
                'queue_size': qsize,
                'is_processing': is_proc,
                'active_analyses': active,
                'worker_last_heartbeat': float(heartbeat) if heartbeat else None,
                'timestamp': time.time()
            }
        except Exception as e:
            logger.warning(f"Redis queue stats okunamadÄ±: {e}")
            return {
                'queue_size': 0,
                'is_processing': False,
                'active_analyses': 0,
                'timestamp': time.time(),
                'error': str(e)
            }
    return {
        'queue_size': analysis_queue.qsize(),
        'is_processing': is_processing,
        'active_analyses': 1 if is_processing else 0,
        'timestamp': time.time()
    }

def cleanup_queue_service():
    """
    Queue service'yi temizle ve background thread'leri durdur
    """
    global is_processing
    
    try:
        logger.info("ðŸ§¹ Queue service cleanup baÅŸlatÄ±lÄ±yor...")

        if is_redis_backend():
            # Redis backend'te cleanup worker prosesin sorumluluÄŸunda.
            logger.info("Redis queue backend aktif: in-process cleanup atlandÄ±.")
            return
        
        # Ä°ÅŸleme durumunu durdur
        with processing_lock:
            is_processing = False
            
        # Kuyruktaki bekleyen iÅŸleri temizle
        while not analysis_queue.empty():
            try:
                analysis_id = analysis_queue.get_nowait()
                logger.info(f"Kuyruktan temizlenen analiz: {analysis_id}")
                analysis_queue.task_done()
            except queue.Empty:
                break
                
        logger.info("âœ… Queue service cleanup tamamlandÄ±!")
        
    except Exception as e:
        logger.error(f"âš ï¸ Queue service cleanup hatasÄ±: {e}") 

def clear_queue():
    """Kuyruktaki tÃ¼m analizleri temizle"""
    global analysis_queue, is_processing
    
    cleared_count = 0

    if is_redis_backend():
        try:
            r = _get_redis()
            # Del -> Ã¶nce uzunluÄŸu al
            cleared_count = int(r.llen(REDIS_QUEUE_KEY) or 0)
            r.delete(REDIS_QUEUE_KEY)
            _set_worker_state(False, 0)
            logger.info(f"Redis kuyruÄŸu temizlendi: {cleared_count} analiz silindi")
            return cleared_count
        except Exception as e:
            logger.error(f"Redis kuyruÄŸu temizlenemedi: {e}")
            return 0
    
    # Ã–nce iÅŸleme durduralÄ±m
    with processing_lock:
        is_processing = False
        
        # Kuyrukta bekleyen tÃ¼m analizleri temizle
        try:
            while True:
                analysis_queue.get_nowait()
                analysis_queue.task_done()
                cleared_count += 1
        except queue.Empty:
            pass
    
    logger.info(f"Kuyruk temizlendi: {cleared_count} analiz silindi")
    return cleared_count