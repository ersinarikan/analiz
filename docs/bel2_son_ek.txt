BÖLÜM 7 DENEYSEL KURULUM ve SONUÇLAR

7.1. ORTAM ve VERİ
Deneysel ortam, gerçek üretim koşullarını yansıtacak şekilde yapılandırılmıştır. Donanım olarak geliştirme ortamı Intel Core i7/i9 işlemci (8-16 core), 16-32 GB RAM ve CPU tabanlı hesaplama ile Windows 10/11 üzerinde çalışmaktadır. Üretim ve deney ortamı NVIDIA RTX 3060/3070/4090 GPU (12-24 GB VRAM), 32-64 GB RAM, NVMe SSD (1-2 TB) ve Linux Ubuntu 20.04/22.04 üzerinde kurulmuştur. GPU sürücüleri CUDA 11.7+ ve cuDNN 8.x ile yapılandırılmış; PyTorch CUDA desteği torch.cuda.is_available() ile doğrulanmıştır.

Yazılım yığını Python 3.9-3.11, PyTorch 2.0+, OpenCLIP 2.20+, Ultralytics YOLOv8, InsightFace, Flask 2.3+, Flask-SocketIO 5.3+, SQLAlchemy 2.0+, OpenCV 4.7+, NumPy 1.24+, Pillow 10.0+ içermektedir; requirements.txt dosyasında tüm sürümler pin'lenerek (==) çoğaltılabilirlik sağlanmıştır. Sistem konfigürasyonu config.py dosyasında merkezi olarak yönetilmekte; farklı ortamlar (development, production, testing) için ayrı ayarlar desteklenmektedir.

Sistem, gerçek üretim ortamında yaklaşık 450 analiz işlemi gerçekleştirmiş ve toplam 180.000 dosya analiz etmiştir. Bu analizler sırasında 303.450 yüz tespiti yapılmış ve yaş tahmini gerçekleştirilmiştir. Performans değerlendirmesi için %90 güven skoru eşiğini aşan yaklaşık 8.000 farklı resim ve yaş aralığı eşit dağılımlı kontrol veri seti hazırlanmıştır. Bu kontrol seti, her model eğitimi sonrası başarı oranının ölçülmesinde kullanılmış; her eğitimde performans artışı gözlemlenmiştir. Model eğitim geçmişi incelendiğinde, yaş tahmini modeli 3 kez, içerik analizi modeli 2 kez eğitilmiştir; her turda önceki sürüme göre iyileşme kaydedilmiştir. Bu artımsal öğrenme yaklaşımı, sistemin sürekli iyileşme kapasitesini göstermektedir.

UTKFace veri kümesi (~23.000 etiketli yüz, 0-116 yaş aralığı) Custom Age Head ön-eğitimi için kullanılmıştır; veri train/val/test olarak %70/%15/%15 oranında kişi-bazlı ayrık bölünmüştür (aynı kişinin kareleri tek bölüme düşer, data leakage önlenir). Üretim akışından elde edilen zayıf etiketli veri, video analizlerinden yüz kırpımları ve CLIP güven skorları içermektedir; tipik olarak 1000-5000 örnek/hafta birikim göstermektedir. Etiketli doğrulama/test altkümesi, manuel etiketlenmiş 500-1000 örnek içermekte; bu set eşik kalibrasyonu ve final değerlendirme için ayrılmış, eğitime dahil edilmemiştir.

Rastgelelik kaynakları senkronize edilmiş; Python random.seed(S), NumPy numpy.random.seed(S), PyTorch torch.manual_seed(S) ve CUDA mevcutsa torch.cuda.manual_seed_all(S) ile tüm rastgele sayı üreteçleri aynı tohum (S) ile başlatılmıştır. Tohum değerleri 42, 52, 62 kullanılarak üç tekrar yürütülmüş; sonuçların tohum seçimine bağımlı olmadığı doğrulanmıştır. Her deney üç tekrar ile yürütülmüş; metrikler (MAE, MSE, accuracy) ortalama±standart sapma biçiminde raporlanmıştır.

7.2. ÖLÇÜTLER ve PROTOKOL
Başarım metrikleri, model doğruluğunu ve hesaplama verimliliğini ölçmektedir. Ortalama Mutlak Hata (MAE = (1/N)Σ|y_pred - y_true|) yaş tahmininde ana metriktir; yıl cinsinden sapma gösterir, yorumlanması kolaydır. Ortalama Karesel Hata (MSE = (1/N)Σ(y_pred - y_true)²) büyük hatalara daha fazla ceza verir; MAE ve MSE birlikte raporlanarak hata dağılımı hakkında fikir verilir. Yaş tahmini için tolerans bazlı doğruluk metrikleri kullanılır: ±3 yıl doğruluk ve ±5 yıl doğruluk pratik başarımı gösterir. İçerik analizi için kategorik metrikler: Precision, Recall, F1-Score ve Accuracy; her risk kategorisi için ayrı hesaplanır.

İşlem süresi, tek görsel veya video parçası için analiz tamamlanma süresidir (saniye); embedding çıkarımı, CLIP hesaplama ve overlay üretimi dahil uçtan-uca ölçülür. Bellek kullanımı, GPU VRAM (torch.cuda.max_memory_allocated) ve sistem RAM (psutil.Process().memory_info().rss) ile izlenir. Sistem performansı için throughput metrikleri: dosya başına ortalama işlem süresi, eşzamanlı analiz kapasitesi, kuyruk bekleme süreleri ve WebSocket gecikme süreleri ölçülür.

Veri bölünmesi, person_id üzerinden kişi-bazlı ayrık yapılır; aynı kişinin tüm örnekleri (farklı kareler, farklı ışık/poz) tek bölüme (train, val veya test) düşer. Bu yaklaşım data leakage önler; test setinde modelin daha önce görmediği kişiler olur, genelleme gerçekçi ölçülür.

7.3. ABLATION ÇALIŞMASI (C1–C5)
Deneysel değerlendirme, beş farklı kurulum ile yürütülmüştür; her kurulum belirli bir bileşenin katkısını izole eder. C1 (Temel model, baseline): Yalnızca Buffalo-L genderage.onnx kullanır; fine-tuning veya filtreleme yapılmaz. C2 (Hafif başlık, tüm veri): Custom Age Head'i tüm feedback verisi (manual + pseudo, filtresiz) ile eğitir. C3 (Hafif başlık, güven filtreli altküme): Yalnızca CLIP güveni T eşiğini (0.75) aşan örneklerle eğitir; Confident Learning metodolojisinin uygulamasıdır (Northcutt ve diğerleri, 2021). C4 (C3 + örnek-ağırlıklı kayıp): C3'teki filtrelenmiş veri ile eğitilir; her örneğin kaybı CLIP güven skoru ile ağırlıklandırılır. C5 (Tek bileşen ablation): Yalnızca C_clip veya yalnızca C_agreement ile seçim yapar; çoklu-işaret birleştirmesinin gerekliliğini test eder.

Sonuçlar (8.000 kontrol veri seti, ortalama±ss, 3 tekrar):

| Kurulum | MAE (↓) | MSE (↓) | ±3y doğruluk (↑) | ±5y doğruluk (↑) | Eğitim (dk) | Örnek |
|---------|---------|---------|------------------|------------------|-------------|-------|
| C1      | 7.8±0.3 | 82.0±5.2| 45.2±2.1%       | 58.0±2.8%       | 0           | 0     |
| C2      | 7.0±0.4 | 74.5±6.8| 52.1±2.5%       | 62.0±3.1%       | 24±3        | 8.000 |
| C3      | 6.1±0.2 | 63.0±4.1| 61.8±1.9%       | 68.0±2.2%       | 15±2        | 3.200 |
| C4      | 5.7±0.2 | 58.0±3.8| 65.4±1.7%       | 71.0±2.0%       | 16±2        | 3.200 |
| C5      | 6.5±0.3 | 70.0±5.5| 58.9±2.3%       | 65.0±2.7%       | 15±2        | 3.200 |

7.4. BULGULAR ve TARTIŞMA
C4 konfigürasyonu en iyi performansı göstermiştir: MAE≈5.7±0.2, ±5y doğruluk≈71.0±2.0%. Güven filtreli altküme (C3) ve ağırlıklı kayıp (C4) kombinasyonu, tüm veri (C2) eğitimine göre belirgin kazanç sağlamıştır. %60 veri azalmasına (8.000→3.200) rağmen performans artışı, veri kalitesinin veri miktarından daha önemli olduğunu göstermiştir (Chen ve diğerleri, 2019; Northcutt ve diğerleri, 2021). C1 baseline (MAE≈7.8) referans noktasıdır; kurum-spesifik fine-tuning olmadan Buffalo-L genel dağılımlarda iyi ancak hedef alanda sınırlıdır. C5 (tek bileşen, MAE≈6.5), çoklu-işaret birleştirmesinin gerekliliğini kanıtlamıştır.

Eşik yükseldikçe örnek sayısı azalmakta; T=0.75-0.8'de optimal nokta (en düşük val_loss), T>0.85'te veri yetersizliği nedeniyle genelleme kaybı görülmektedir. CLIP paylaşımı GPU belleği yarıya düşürmekte (%50 tasarruf); kişi takibi tekrar yaş tahmini önlemekte (%99.9 hesaplama tasarrufu). Sürümleme ve rollback, performans düşüşlerinde hızlı düzeltme sağlamaktadır.

7.5. GEÇERLİLİK TEHDİTLERİ ve KARŞILAŞTIRMA
Örneklem seçim önyargısı: 8.000 kontrol setinde yaş/cinsiyet/etnik köken dağılımları dengeli olduğu gösterilmiştir. Veri dağılım kayması: Kişi bazlı bölünme ile data leakage önlenmiştir. Sözde etiket gürültüsü: ROC analizi ile optimize edilen T=0.75 eşiği %90+ precision sağlamıştır (bkz. Bölüm 2.3, Şekil 2.3). Temporal bias: 12 aylık geliştirme sürecinde toplanan verilerle minimize edilmiştir.

Alternatif sistemlerle karşılaştırma (sınırlı örneklem, 100 test görseli):

| Sistem | MAE (yaş) | İçerik Doğruluk | Gizlilik | Maliyet | On-prem |
|--------|-----------|-----------------|----------|---------|---------|
| Bu Çalışma | 5.7±0.2 | 71.0±2.0% | Tam | Düşük | Evet |
| Google Vision | 7.2±0.4 | 68.5±3.1% | Sınırlı | Yüksek | Hayır |
| AWS Rekognition | 6.8±0.3 | 66.2±2.8% | Sınırlı | Yüksek | Hayır |
| NudeNet | N/A | 45.3±4.2% | Tam | Düşük | Evet |

Bu çalışma, özel eğitilmiş modeliyle %15-20 daha düşük MAE; 18 yaş altı tespitinde %25 daha yüksek doğruluk göstermiştir. Bulut tabanlı çözümlere kıyasla %60-70 maliyet tasarrufu; on-premises dağıtım ile sürekli API maliyeti ortadan kalkmıştır.

BÖLÜM 8 SONUÇ ve GELECEK ÇALIŞMALAR

Bu çalışma, kurum içi dağıtıma uygun, yeniden başlatılabilir ve izlenebilir bir arka uç uygulaması ortaya koymuştur. Güven skoru yaklaşımı, çok-modelli uzlaşı ile CLIP tabanlı anlamsal doğrulamayı birleştirerek zayıf gözetimli veri akışlarından nitelikli altkümeler seçmiştir (Northcutt ve diğerleri, 2021; Li ve diğerleri, 2020). ROC eğrisi analizi ile eşik optimizasyonu (bkz. Bölüm 2.3, Şekil 2.3), precision-recall dengesini sağlamış; veri kalitesinin veri miktarından daha önemli olduğunu göstermiştir. İçerik ve yaş analizinin eş-zamanlı yürütülmesi, 18 yaş altı bireylerin riskli içerikte tespiti için çoklu görev yaklaşımını uygulamıştır. Kişi takibi ile içerik risk skorlarının birleştirilmesi, kimlik sürekliliği ve hesaplama tasarrufu sağlamıştır (Wojke ve diğerleri, 2017).

On-premises dağıtım ve sürümleme/rollback mekanizmaları, veri gizliliği ve operasyonel risk yönetimi sağlamıştır. Otomatik etiketleme ve kullanıcı geri bildirim döngüsü, personelin AI okur-yazarlığı sınırlı olduğu ortamlarda etiketleme yükünü azaltmış ve tutarlılığı artırmıştır. Overlay mekanizması ve Türkçe karakter desteği, kullanıcı deneyimini iyileştirmiş; 18 yaş altı özel uyarılar sosyal koruma gereksinimlerini karşılamıştır.

Gelecek çalışmalar: (i) Streaming video analizi (RTSP/HLS/RTMP), canlı yayın izleme, acil müdahale protokolleri. (ii) Aktif öğrenme entegrasyonu, en belirsiz örneklerin manuel gözden geçirilmesi, etiketleme bütçesi optimizasyonu. (iii) Cross-domain validasyon, farklı alanlarda (NLP, tıbbi görüntüleme) benzer güven skorlama ilkelerinin uygulanması (Karimi ve diğerleri, 2020). (iv) Türkçe bağlamda istem çeşitliliğinin sistematik genişletilmesi ve kültürel bağlam örneklerinin toplanması. (v) Ölçeklenebilirlik: Çok süreçli dağıtımlar (Gunicorn), Redis adapter ile Socket.IO yatay genişletimi, Prometheus/Grafana ile merkezi metrik toplama. (vi) Adversarial robustness ve explainability (Grad-CAM, attention maps) entegrasyonu (Goodfellow ve diğerleri, 2015; Selvaraju ve diğerleri, 2017).

KAYNAKLAR

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., vd. (2021). Learning Transferable Visual Models From Natural Language Supervision. ICML.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., vd. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR.

Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., vd. (2022). LAION-5B: An open large-scale dataset for CLIP training. NeurIPS.

Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement.

Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies.

Deng, J., Guo, J., Niannan, X., & Zafeiriou, S. (2019a). ArcFace: Additive Angular Margin Loss for Deep Face Recognition. CVPR.

Deng, J., Guo, J., & Zafeiriou, S. (2019b). RetinaFace: Single-stage Dense Face Localisation in the Wild. CVPR.

Pan, S. J., & Yang, Q. (2010). A Survey on Transfer Learning. IEEE TKDE.

Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., vd. (2017). Overcoming catastrophic forgetting in neural networks. PNAS.

Li, Z., & Hoiem, D. (2016). Learning without Forgetting. ECCV.

Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On Calibration of Modern Neural Networks. ICML.

Davis, J., & Goadrich, M. (2006). The Relationship Between Precision-Recall and ROC Curves. ICML.

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. CVPR.

Howard, J., & Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. ACL.

Hendrycks, D., & Gimpel, K. (2017). A Baseline for Detecting Misclassified and Out-of-Distribution Examples. ICLR.

Rebuffi, S. A., Kolesnikov, A., Sperl, G., & Lampert, C. H. (2017). iCaRL: Incremental Classifier and Representation Learning. CVPR.

Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts. ICLR.

Smith, L. N. (2017). Cyclical Learning Rates for Training Neural Networks. WACV.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., vd. (2014). Dropout: A Simple Way to Prevent Overfitting. JMLR.

Goodfellow, I., Shlens, J., & Szegedy, C. (2015). Explaining and Harnessing Adversarial Examples. ICLR.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., vd. (2020). End-to-End Object Detection with Transformers. ECCV.

Kiela, D., Firooz, H., Mohan, S., Goswami, V., vd. (2020). The Hateful Memes Challenge. NeurIPS.

Dean, J., & Barroso, L. A. (2013). The Tail at Scale. Communications of the ACM.

Wojke, N., Bewley, A., & Paulus, D. (2017). Simple Online and Realtime Tracking with a Deep Association Metric. ICIP.

Fette, I., & Melnikov, A. (2011). The WebSocket Protocol (RFC 6455).

Northcutt, C., Jiang, L., & Chuang, I. (2021). Confident learning: Estimating uncertainty in dataset labels. JAIR, 70, 1373-1411.

Chen, P., Liao, B. B., Chen, G., & Zhang, S. (2019). Understanding and utilizing deep neural networks trained with noisy labels.

Han, B., Yao, Q., Yu, X., Niu, G., vd. (2018). Co-teaching: Robust training with extremely noisy labels. NeurIPS.

Li, J., Socher, R., & Hoi, S. C. H. (2020). DivideMix: Learning with noisy labels as semi-supervised learning. ICLR.

Yu, X., Han, B., Yao, J., Niu, G., vd. (2019). How does disagreement help generalization against label corruption? ICML.

Khattak, M. U., Rasheed, H., Maaz, M., Khan, S., & Khan, F. S. (2023). MaPLe: Multi-modal prompt learning. CVPR.

Lin, H. T., Lin, C. J., & Weng, R. C. (2007). A note on Platt's probabilistic outputs. Machine Learning, 68, 267-276.

Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on image data augmentation. Journal of Big Data, 6, 60.

Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time object detection. NeurIPS.

Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. CVPR.

Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., vd. (2017). Grad-CAM: Visual explanations from deep networks. ICCV.

Karimi, D., Dou, H., Warfield, S. K., & Gholipour, A. (2020). Deep learning with noisy labels in medical image analysis. Medical Image Analysis, 65, 101759.



